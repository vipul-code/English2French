{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Vipul\n",
      "[nltk_data]     Kapadia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import operator\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, TimeDistributed, RepeatVector, Embedding, Input, LSTM, Conv1D, MaxPool1D, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style(theme='monokai', context='notebook', ticks=True, grid=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-357775e45c9e>:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df_english = pd.read_csv(r'F:\\Coursera\\Projects\\lstm_translator\\small_vocab_en.csv', sep = '/t', names = ['english'])\n",
      "<ipython-input-3-357775e45c9e>:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df_french = pd.read_csv(r'F:\\Coursera\\Projects\\lstm_translator\\small_vocab_fr.csv', sep = '/t', names = ['french'])\n"
     ]
    }
   ],
   "source": [
    "df_english = pd.read_csv(r'F:\\Coursera\\Projects\\lstm_translator\\small_vocab_en.csv', sep = '/t', names = ['english'])\n",
    "df_french = pd.read_csv(r'F:\\Coursera\\Projects\\lstm_translator\\small_vocab_fr.csv', sep = '/t', names = ['french'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>french</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>new jersey is sometimes quiet during autumn , ...</td>\n",
       "      <td>new jersey est parfois calme pendant l' automn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the united states is usually chilly during jul...</td>\n",
       "      <td>les Ã©tats-unis est gÃ©nÃ©ralement froid en ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>california is usually quiet during march , and...</td>\n",
       "      <td>california est gÃ©nÃ©ralement calme en mars , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the united states is sometimes mild during jun...</td>\n",
       "      <td>les Ã©tats-unis est parfois lÃ©gÃ¨re en juin ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>your least liked fruit is the grape , but my l...</td>\n",
       "      <td>votre moins aimÃ© fruit est le raisin , mais m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137855</th>\n",
       "      <td>france is never busy during march , and it is ...</td>\n",
       "      <td>la france est jamais occupÃ©e en mars , et il ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137856</th>\n",
       "      <td>india is sometimes beautiful during spring , a...</td>\n",
       "      <td>l' inde est parfois belle au printemps , et il...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137857</th>\n",
       "      <td>india is never wet during summer , but it is s...</td>\n",
       "      <td>l' inde est jamais mouillÃ© pendant l' Ã©tÃ© ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137858</th>\n",
       "      <td>france is never chilly during january , but it...</td>\n",
       "      <td>la france est jamais froid en janvier , mais i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137859</th>\n",
       "      <td>the orange is her favorite fruit , but the ban...</td>\n",
       "      <td>l'orange est son fruit prÃ©fÃ©rÃ© , mais la ba...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>137860 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  english  \\\n",
       "0       new jersey is sometimes quiet during autumn , ...   \n",
       "1       the united states is usually chilly during jul...   \n",
       "2       california is usually quiet during march , and...   \n",
       "3       the united states is sometimes mild during jun...   \n",
       "4       your least liked fruit is the grape , but my l...   \n",
       "...                                                   ...   \n",
       "137855  france is never busy during march , and it is ...   \n",
       "137856  india is sometimes beautiful during spring , a...   \n",
       "137857  india is never wet during summer , but it is s...   \n",
       "137858  france is never chilly during january , but it...   \n",
       "137859  the orange is her favorite fruit , but the ban...   \n",
       "\n",
       "                                                   french  \n",
       "0       new jersey est parfois calme pendant l' automn...  \n",
       "1       les Ã©tats-unis est gÃ©nÃ©ralement froid en ju...  \n",
       "2       california est gÃ©nÃ©ralement calme en mars , ...  \n",
       "3       les Ã©tats-unis est parfois lÃ©gÃ¨re en juin ,...  \n",
       "4       votre moins aimÃ© fruit est le raisin , mais m...  \n",
       "...                                                   ...  \n",
       "137855  la france est jamais occupÃ©e en mars , et il ...  \n",
       "137856  l' inde est parfois belle au printemps , et il...  \n",
       "137857  l' inde est jamais mouillÃ© pendant l' Ã©tÃ© ,...  \n",
       "137858  la france est jamais froid en janvier , mais i...  \n",
       "137859  l'orange est son fruit prÃ©fÃ©rÃ© , mais la ba...  \n",
       "\n",
       "[137860 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#concat both the dataframes\n",
    "# axis = 0 will add french data below english data, axis =1 will add french data in next column\n",
    "\n",
    "df_new = pd.concat([df_english, df_french], axis = 1)\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total english records = 137860\n",
      "total french records = 137860\n"
     ]
    }
   ],
   "source": [
    "# to print number of records\n",
    "#len(which df(which column))\n",
    "\n",
    "print('total english records = {}'.format(len(df_new['english'])))\n",
    "print('total french records = {}'.format(len(df_new['french'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Task#3 : Perform data cleaning\n",
    "#remove punctuations by creating a function\n",
    "#re.sub is regualr expression. substitute\n",
    "\n",
    "def remove_punc(x):\n",
    "    return re.sub('[!#?,.:\";]', '', x)\n",
    "\n",
    "df_new['english'] = df_new['english'].apply(remove_punc)\n",
    "df_new['french'] = df_new['french'].apply(remove_punc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199\n",
      "350\n"
     ]
    }
   ],
   "source": [
    "english_words = []\n",
    "french_words = []\n",
    "\n",
    "\n",
    "#create a function to add only unique words to list\n",
    "#x will pass on each row.\n",
    "#x.split will split the sentense in words\n",
    "\n",
    "def get_unique_words(x, words_list):\n",
    "    for word in x.split:\n",
    "        if word not in words_list:\n",
    "            words_list.append(word)\n",
    "            \n",
    "\n",
    "english_words = list(df_new['english'].str.split(' ', expand=True).stack().unique())\n",
    "french_words = list(df_new['french'].str.split(' ', expand=True).stack().unique())        \n",
    "\n",
    "#for word in df_new['english'].str.split(' '):\n",
    " #   if word not in english_words:\n",
    " #       english_words.append(word)\n",
    "\n",
    "total_english = len(english_words) - 1\n",
    "print(total_english)\n",
    "\n",
    "total_french = len(french_words) - 1\n",
    "\n",
    "print(total_french)\n",
    "\n",
    "#df_new['english'].apply(lambda x: get_unique_words(x, english_words))\n",
    "#df_new['french'].apply(lambda x: get_unique_words(x, french_words))\n",
    "\n",
    "\n",
    "#total_english = len(english_words)\n",
    "#print(total_english)\n",
    "\n",
    "#total_french = len(french_words)\n",
    "#print(total_french)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum number of words in english document =  15\n",
      "The maximum number of words in french document =  24\n"
     ]
    }
   ],
   "source": [
    "# Maximum length (number of words) per document. We will need it later for embeddings\n",
    "maxlen_english = -1\n",
    "for doc in df_new.english:\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    if(maxlen_english < len(tokens)):\n",
    "        maxlen_english = len(tokens)\n",
    "print(\"The maximum number of words in english document = \", maxlen_english)\n",
    "\n",
    "maxlen_french = -1\n",
    "for doc in df_new.french:\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    if(maxlen_french < len(tokens)):\n",
    "        maxlen_french = len(tokens)\n",
    "print(\"The maximum number of words in french document = \", maxlen_french)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete English Vocab Size: 200\n",
      "Complete French Vocab Size: 351\n"
     ]
    }
   ],
   "source": [
    "#Tokenization of data\n",
    "\n",
    "#tokenizer allows us to vetorize the data by converting words to sequence of integers\n",
    "\n",
    "def tokenize_and_pad(x, maxlen):\n",
    "    tokenizer = Tokenizer(char_level=False)\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    sequence = tokenizer.texts_to_sequences(x)\n",
    "    padded = pad_sequences(sequence, maxlen = maxlen, padding= 'post')\n",
    "    return tokenizer, sequence, padded\n",
    "\n",
    "x_tokernizer, x_sequences, x_padded = tokenize_and_pad(df_new.english, maxlen_english)\n",
    "y_tokernizer, y_sequences, y_padded = tokenize_and_pad(df_new.french, maxlen_french)\n",
    "\n",
    "# Total vocab size, since we added padding we add 1 to the total word count\n",
    "english_vocab_size = total_english + 1\n",
    "print(\"Complete English Vocab Size:\", english_vocab_size)\n",
    "\n",
    "# Total vocab size, since we added padding we add 1 to the total word count\n",
    "french_vocab_size = total_french + 1\n",
    "print(\"Complete French Vocab Size:\", french_vocab_size)\n",
    "\n",
    "# function to obtain the text from padded variables\n",
    "def pad_to_text(padded, tokenizer):\n",
    "\n",
    "    id_to_word = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    id_to_word[0] = ''\n",
    "\n",
    "    return ' '.join([id_to_word[j] for j in padded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_padded, y_padded, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 15, 256)           51200     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 24, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 24, 256)           525312    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 24, 351)           90207     \n",
      "=================================================================\n",
      "Total params: 1,192,031\n",
      "Trainable params: 1,192,031\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#building model\n",
    "#we are going to use keras and tensorflow.\n",
    "#keras is very powerful. we do not have to know LSTM functioning. we can \n",
    "#just call LSTM and say we need 128 units\n",
    "\n",
    "# Sequential Model\n",
    "model = Sequential()\n",
    "# embedding layer\n",
    "model.add(Embedding(english_vocab_size, 256, input_length = maxlen_english, mask_zero = True))\n",
    "# encoder\n",
    "model.add(LSTM(256))\n",
    "# decoder\n",
    "# repeatvector repeats the input for the desired number of times to change\n",
    "# 2D-array to 3D array. For example: (1,256) to (1,23,256)\n",
    "model.add(RepeatVector(maxlen_french))\n",
    "model.add(LSTM(256, return_sequences= True ))\n",
    "model.add(TimeDistributed(Dense(french_vocab_size, activation ='softmax')))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the shape of target from 2D to 3D\n",
    "y_train = np.expand_dims(y_train, axis = 2)\n",
    "y_train.shape\n",
    "\n",
    "# train the model\n",
    "model.fit(x_train, y_train, batch_size=1024, validation_split= 0.1, epochs=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save(\"weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to make prediction\n",
    "def prediction(x, x_tokenizer = x_tokernizer, y_tokenizer = y_tokernizer):\n",
    "    predictions = model.predict(x)[0]\n",
    "    id_to_word = {id: word for word, id in y_tokenizer.word_index.items()}\n",
    "    id_to_word[0] = ''\n",
    "    return ' '.join([id_to_word[j] for j in np.argmax(predictions,1)])\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "  print('Original English word - {}\\n'.format(pad_to_text(x_test[i], x_tokernizer)))\n",
    "  print('Original French word - {}\\n'.format(pad_to_text(y_test[i], y_tokernizer)))\n",
    "  print('Predicted French word - {}\\n\\n\\n\\n'.format(prediction(x_test[i:i+1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
